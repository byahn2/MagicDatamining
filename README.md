# MagicDatamining
Data Mining Final Project
Bryce Elizabeth Yahn and Jeremy Atkins

Dataset Choice:
The dataset we‚Äôve chosen to work with is the Magic Gamma Telescope (Magic) dataset from the UCI Machine Learning repository,  generated by Bock et. al. for their paper Methods for multidimensional event classification: a case study using images from a Cherenkov gamma-ray telescope. The authors used a Monte Carlo simulation to generate particle showers as caused both by high-energy gamma radiation and background cosmic ray activity, as seen by ground-based Cherenkov telescopes. While ground-based telescopes offer the advantage of a much larger collection area, allowing them to respond better to lower fluxes, they also run the risk of swamping the interesting gamma ray data with uninteresting hadronic showers caused by background cosmic rays. Our objective is to pick out the shower events caused by high-energy gamma rays based on parameters related to their particle showers.
Magic has 19020 instances, each with 10 continuous features describing parameters of the particle shower.  There are two categorical target classes (gamma radiation and background radiation), making this a binary classification task.  We chose this dataset in part because it is related to one of our majors, and in part because it has both a large number of attributes and a large number of instances.  We hope this makes for an interesting analysis.
https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope 

Goal of the Analysis:
Our goal is to create a model that can classify each item in the dataset as caused by gamma radiation or cosmic rays.  This is a supervised learning classification task.

Planned Technical Approach

1 Data Visualization
We will first familiarize ourselves with the data and attempt to visualize it, which will mostly involve plotting distributions of the various attributes, as well as looking at correlations. There are many attributes, and having a relative understanding of the underlying distributions of these attributes may help us tune our methods to better fit our data (such as picking the right kernel). We may also want to visualize some of the particle collisions, if that becomes necessary.

2 Data Preprocessing
There are no missing values in this dataset, but we will still want to go through and normalize the data.  We believe that all attributes are ratio-scaled, so we will either normalize each attribute to be within a given range (0,1) or, based on our visualization, a normal distribution (ùùÅ=0,ùö∫=I).  Additionally, there is no separate test case, so we will have to divide the data into a training set and a test set in such a way that both are representative samples of the whole dataset.  The dataset is large enough that we should be able to do an 80-20% split, however, if we later decide this is not enough, we may choose to do k-fold cross validation.

3 Model Selection
We plan on using Naive Bayes as the baseline, and support vector machines as our more sophisticated model.  The Naive Bayes model makes the often untrue assumption that each attribute is independent (and, according to a preliminary report on the data, there‚Äôs at least two columns that are highly correlated), but it still performs well in many situations.  Support vector machines are better equipped for handling dependencies, especially with so many features.  If there are interactions between attributes, SVM should produce a better classification.  That being said, it‚Äôs possible that naive Bayes will be sufficient for classification in this dataset.  It will be interesting to compare their performance.


4 Model Evaluation
We will likely use a ROC to evaluate the performance of our models, which plots the false positive rate against the true positive rate; the area under the ROC is generally a good signifier of classifier performance. However, if the dataset is significantly unbalanced, ROC will be insufficient, and we will likely use a precision-recall curve, whose area functions similarly.
